= Transformer-Based Hyphenation

:author: Froldas

== Getting started

Python version >= 3.12 required.

`pip install -r .`

dot (needed by graphviz) is expected in your path

== About

Hyphenation using transformers

=== How to use

TRAIN: `train.py`

Runs training of the model and dumps the network files with its configuration plus performs evaluation.

GRID_RUN: `grid_run.py`
Alternatively, there is script `grid_run.py` where you can select which configuration superset will be run
in a sequence.

== Configuration space
Using `configuration.yml` you can control what model architecture, encoding, learning rate and other parameters are used during all stages.

Here is the list of the most important ones:

=== *model*

`SimpleMLP` - regular fully connected multi-layer perceptron model

`SimpleTransformer` - self-attention layer followed by fully connected model

...
Check src/ConfDict.py to see all available models

=== *encoding*

`binary` - letters are converted to a binary number based on their index

`one_hot` - letters are converted to a one-hot vector based on their index

`simple_float` - letters are converted to a single float with even spacing

`advanced_float` - letters are converted to a single float with vowels and consonants grouped



== Notes
Oxford dictionary 500 is a merge of oxford 3000 and oxford 5000 etra 2000 words, but I had to clean non-letter symbols,
compound phrases and duplication marks

